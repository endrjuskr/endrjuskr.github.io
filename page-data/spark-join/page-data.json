{"componentChunkName":"component---src-templates-blog-post-js","path":"/spark-join/","result":{"data":{"markdownRemark":{"id":"b1ed09a1-5a20-5010-97e1-3a8fb3f4d76b","excerpt":"Reason for this topis is following code: It caused out of memory exception and encouraged me to dig deeper into what happened. To make my points clear, here areâ€¦","html":"<p>Reason for this topis is following code:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">  df <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">20000</span><span class=\"token punctuation\">)</span>\n  another_df <span class=\"token operator\">=</span> another_df<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">20000</span><span class=\"token punctuation\">)</span>\n\n  df <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>\n    df\n    join<span class=\"token punctuation\">.</span><span class=\"token punctuation\">(</span>another_df<span class=\"token punctuation\">,</span> <span class=\"token string\">\"key\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"inner\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">)</span></code></pre></div>\n<p>It caused out of memory exception and encouraged me to dig deeper into what happened.</p>\n<p>To make my points clear, here are details of my setup.</p>\n<p>Spark configuration relevant to my cases:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">spark.sql.autoBroadcastJoinThreshold -1 // Prevent automatic broadcast\nspark.sql.shuffle.partitions <span class=\"token number\">200</span> // Default partitions during shuffle.</code></pre></div>\n<p>Three datasets:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">large_df<span class=\"token punctuation\">.</span>columns\nOut<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'category'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'no'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'value'</span><span class=\"token punctuation\">]</span>\n\nlarge_df <span class=\"token operator\">=</span> large_df<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"no\"</span><span class=\"token punctuation\">)</span>\nlarge_df<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n\nanother_large_df<span class=\"token punctuation\">.</span>columns\nOut<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'category_2'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'no'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'value_2'</span><span class=\"token punctuation\">]</span>\n\nlarge_df <span class=\"token operator\">=</span> large_df<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"no\"</span><span class=\"token punctuation\">)</span>\nlarge_df<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">10</span>\n\nmapping<span class=\"token punctuation\">.</span>columns\nOut<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'category'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'category_name'</span><span class=\"token punctuation\">]</span>\n\nmapping <span class=\"token operator\">=</span> mapping<span class=\"token punctuation\">.</span>coalesce<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nmapping<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">1</span></code></pre></div>\n<h2>Initial observation</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">join_different_partitions_keys <span class=\"token operator\">=</span> large_df<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>another_large_df<span class=\"token punctuation\">,</span> <span class=\"token string\">\"no\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"inner\"</span><span class=\"token punctuation\">)</span>\njoin_different_partitions_keys<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">200</span></code></pre></div>\n<p>Original dataframes <code class=\"language-text\">large_df</code> and <code class=\"language-text\">another_large_df</code> had 10 partitions each, but output is 200. The reason for it is shuffling. Both dataframes are having 10 partitions but Spark does not know what data is inside each partition of <code class=\"language-text\">another_large_df</code> in terms of <code class=\"language-text\">no</code>. More importantly, does not which <code class=\"language-text\">no</code>s are there. To avoid comparing every pair combination, Spark performs shuffle, which moves rows between partitions to have a notion of what <code class=\"language-text\">no</code>s are in each of them. It shuffles dataframe to 200 partitions instead of 10. 200 is coming from <code class=\"language-text\">spark.sql.shuffle.partitions</code>. </p>\n<p>In my original situation trying fit 20000 partitions into 200 resulted in out of memory on an executor. I realized it when I noticed 200 tasks in Spark UI while executing join.</p>\n<h2>More observations</h2>\n<p>Same observation when we have longer partition key:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">another_large_df <span class=\"token operator\">=</span> another_large_df<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"no\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"category_2\"</span><span class=\"token punctuation\">)</span>\njoin_longer_partitions_keys <span class=\"token operator\">=</span> large_df<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>another_large_df<span class=\"token punctuation\">,</span> <span class=\"token string\">\"no\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"inner\"</span><span class=\"token punctuation\">)</span>\njoin_longer_partitions_keys<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">200</span></code></pre></div>\n<p>Similar observation in case of mismatch between partition key and join key:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">another_large_df <span class=\"token operator\">=</span> another_large_df<span class=\"token punctuation\">.</span>withColumnRenamed<span class=\"token punctuation\">(</span><span class=\"token string\">\"category_2\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"category\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"no\"</span><span class=\"token punctuation\">)</span>\njoin_longer_join_key <span class=\"token operator\">=</span> large_df<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>another_large_df<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"no\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"category\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"inner\"</span><span class=\"token punctuation\">)</span>\njoin_longer_join_key<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">200</span></code></pre></div>\n<p>In case of matching keys we are receiving expected result:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">another_large_df <span class=\"token operator\">=</span> another_large_df<span class=\"token punctuation\">.</span>withColumnRenamed<span class=\"token punctuation\">(</span><span class=\"token string\">\"category\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"category_2\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"no\"</span><span class=\"token punctuation\">)</span>\noptimal_join <span class=\"token operator\">=</span> large_df<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>another_large_df<span class=\"token punctuation\">,</span> <span class=\"token string\">\"no\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"inner\"</span><span class=\"token punctuation\">)</span>\noptimal_join<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">10</span></code></pre></div>\n<h2>Broadcast observations</h2>\n<p>Similar issue we experience in case of small data:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">not_broadcast_join <span class=\"token operator\">=</span> large_df<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>mapping<span class=\"token punctuation\">,</span> <span class=\"token string\">\"category\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"inner\"</span><span class=\"token punctuation\">)</span>\nnot_broadcast_join<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">200</span></code></pre></div>\n<p>Broadcast solves the issue:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">broadcast_join <span class=\"token operator\">=</span> large_df<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>F<span class=\"token punctuation\">.</span>broadcast<span class=\"token punctuation\">(</span>mapping<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"category\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"inner\"</span><span class=\"token punctuation\">)</span>\nbroadcast_join<span class=\"token punctuation\">.</span>rdd<span class=\"token punctuation\">.</span>getNumPartitions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nOut<span class=\"token punctuation\">:</span> <span class=\"token number\">10</span></code></pre></div>\n<h2>Conclusion</h2>\n<p>Here are some things to remember</p>\n<ul>\n<li>use <code class=\"language-text\">broadcast</code> in case of joining with small dataframes, Spark automatically does it if it is below <code class=\"language-text\">spark.sql.autoBroadcastJoinThreshold</code></li>\n<li>know set <code class=\"language-text\">spark.sql.shuffle.partitions</code></li>\n<li>keep the same partition and join keys</li>\n</ul>\n<p>Entire code can be found <a href=\"https://gist.github.com/endrjuskr/81c401a8825f929ecd1125018e71fbbe\">here</a></p>","frontmatter":{"title":"Undestanding Spark's joins","date":"May 01, 2020","description":"Notes about Spark's joins to avoid OOM in future"}}},"pageContext":{"slug":"/spark-join/","previous":null,"next":null}}}